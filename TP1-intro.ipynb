{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of NLP: DataFrame rudiments and spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries \n",
    "\n",
    "We will consider a corpus of radio recordings in french (source: France inter, RFI, ...) :  RADIOS.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in writing python code is determining which libraries we want to use. To stay organized, we generally import all libraries at the top (of a notebook or a standalone python file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import four important libraries\n",
    "import pandas as pd\n",
    "#pandas is the most popular data science library. It is used\n",
    "#for organizing data into columns.\n",
    "#See more here: https://pandas.pydata.org/\n",
    "\n",
    "import re\n",
    "#re is the python library for regular expressions. It is\n",
    "#difficult to master, but very useful.\n",
    "#See more here: https://docs.python.org/3/library/re.html\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib is the most popular and simple library for\n",
    "#scientific plots in python. The pyplot submodule is\n",
    "#a data scientist's bread and butter for data visualization.\n",
    "#See more here: https://matplotlib.org/2.0.2/api/pyplot_api.html\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "#ElementTree is a library for parsing XML in python.\n",
    "#This is a useful skill for data scientists, as lots of data\n",
    "#is published in XML form. There are other libraries for this,\n",
    "#such as untangle or xmltodict; however, ElementTree is lower level.\n",
    "#See more here: https://docs.python.org/3/library/xml.etree.elementtree.html\n",
    "\n",
    "from numpy import log\n",
    "#we will need a logarithm function; numpy is a standard mathematical\n",
    "#library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Dataframe basics\n",
    "\n",
    "In this TP we use Pandas [DataFrames](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) to manage our dataset. Pandas is a standard tool in data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the dataset as a DataFrame\n",
    "df_radios = pd.read_csv('RADIOS.TXT', names=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_radios))\n",
    "df_radios.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can list the columns of a dataset ````df```` by calling ````df.columns````.\n",
    "\n",
    "**Question**: What are the columns of the dataset ````df_radios````?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_radios.columns\n",
    "#todo: explain each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access to a specific using ````df.loc[i]```` or ````df.iloc[i]```` \n",
    "\n",
    "\n",
    "**Question**: What is the difference between .loc and .iloc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: save the text value of the 5th element of our dataset into the one_text variable. \n",
    "one_text = '' #todo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can count the number of words in a string using the ````len```` function and ````split```` method of strings. For example:\n",
    "\n",
    "\n",
    "```\n",
    "string = \"bla bla bla\"\n",
    "len(string.split(\" \"))\n",
    ">>> 3\n",
    "```\n",
    "\n",
    "**Question**: Fill in the following function and run it for a our ````one_text```` example. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_words(sentence):\n",
    "    '''\n",
    "    Calculate the number of words in the sentence.\n",
    "    '''\n",
    "    #todo\n",
    "\n",
    "num_words(one_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Furthermore, we can calculate values for each row, based on a specific column, using the ````apply```` method:\n",
    "\n",
    "```\n",
    "lengths = df.column_name.apply(lambda text: num_words(text))\n",
    "```\n",
    "\n",
    "This will give us a [pandas Series](https://pandas.pydata.org/docs/reference/api/pandas.Series.html) of the lengths of each text.\n",
    "\n",
    "**Question**: Add the length of each text as a new column in the dataframe (see [pandas docs](https://pandas.pydata.org/docs/user_guide/dsintro.html#basics-dataframe-sel-add-del) for details). \n",
    "\n",
    "**Question**: What is the distribution of the length of the texts in words? Visualize this using a python plot (see [plt.hist](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "\n",
    "lengths = '' #todo \n",
    "\n",
    "## to plot \n",
    "plt.hist(lengths,bins=100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter a dataframe using criteria based on values in certain columns. The basic syntax for this is:\n",
    "\n",
    "````\n",
    "new_df = df[df[\"column\"].apply(lambda row_value: my_func(row_value) == some_value)]\n",
    "````\n",
    "\n",
    "This gives a new DataFrame composed of all elements of ````df```` such the values of the function ````myfunc````, calculated for the value of column \"column\" for each row is equal to some_value. For example, if we wanted all rows that begin with \"a\", we could run:\n",
    "\n",
    "````\n",
    "df_letter_A = df[df[\"text\"].apply(lambda text: text[0] == \"a\")]\n",
    "````\n",
    "\n",
    "**Question**: Retain only documents with at least 50 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding useful information in the text:\n",
    "\n",
    "We will implement regular expresions (regex) to filter the text according to a specific condiction. \n",
    "\n",
    "To implement the regex evaluation in Pandas we use the [`Series.str.contains`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.contains.html) filtering function, as :\n",
    "\n",
    "```` df.column_name.str.contains('[a-z]', regex=True)````\n",
    "\n",
    "Also useful: `match`, `Series.str.startswith`, `Series.str.endswith`\n",
    "\n",
    "\n",
    "**Question**  : How many texts are there with at least one word of 9 letters in RADIOS.txt?\n",
    "\n",
    "**Question**  : Are there any texts with words without vowels in RADIOS.txt?\n",
    "\n",
    "\n",
    "You can use the following regexs : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| regex | Description |\n",
    "|--------|--------------------|\n",
    "|`'[A-Z]'`|filtre lignes contenant une majuscule|\n",
    "|`'^[A-Z]'`|filtre lignes commençant par une majuscule|\n",
    "|`'[A-Z]$'`|filtre lignes finissant par une majuscule|\n",
    "|`'^[A-Z]*$'`|filtre lignes entièrement majuscules|\n",
    "|`'[aeiouAEIOU]'`|filtre lignes contenant une voyelle|\n",
    "|`'^[aeiouAEIOU]'`|filtre lignes commençant par une voyelle|\n",
    "|`'[aeiouAEIOU]$'`|filtre lignes finissant par une voyelle|\n",
    "|`'^[^aeiouAEIOU]'`|filtre lignes commençant par une non-voyelle|\n",
    "|`'[^aeiouAEIOU]$'`|filtre lignes finissant par une non-voyelle|\n",
    "|`'[aeiouAEIOU].*[aeiouAEIOU]'`|filtre lignes avec au moins deux voyelles|\n",
    "|`'^[^aeiouAEIOU]*[aeiouAEIOU][^aeiouAEIOU]*$'`| filtre lignes avec exactement une voyelle|\n",
    "\n",
    "Avec expressions régulières:\n",
    "\n",
    "| Expression | Match |\n",
    "|--------|--------------------|\n",
    "|`a`|la lettre \"a\"|\n",
    "|`[a-z]`|une lettre minuscule|\n",
    "|`[A-Z]`|une lettre majuscule|\n",
    "|`[0-9]`|un chiffre|\n",
    "|`[0123456789]`|un chiffre|\n",
    "|`[aeiouAEIOU]`|une voyelle|\n",
    "|`[^aeiouAEIOU]`|tout sauf une voyelle|\n",
    "|`.`|un caractère|\n",
    "|`^`|début de ligne|\n",
    "|`$`|fin de ligne|\n",
    "|`x*`| \"x\" répété 0 fois ou plus|\n",
    "|``x+``| \"x\" répété 1 fois ou plus|\n",
    "|`x\\|y`| \"x\" ou \"y\"|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For example: \n",
    "df_radios['a-z'] = df_radios.text.str.contains('[a-z]', regex=True)\n",
    "df_radios[df_radios['a-z']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary \n",
    "\n",
    "We will create a a dictionary as the vocabulary for our corpus of texts. \n",
    "Create a dictionary where each key is a word and the value will be its frequency. \n",
    "\n",
    "```` texts_vocab = {'key1':value, 'key2':value} ````\n",
    "\n",
    "**Question** Create a Vocabulary of bigrams and trigrams \n",
    "\n",
    "**Question** How many different words exist in our corpus of texts?\n",
    "\n",
    "**Question**: What is the frequency distribution of the words? use ```plt.hist```\n",
    "\n",
    "**Question** [optional] How many bigrams and trigrams exist? Visualize the frequency distribution. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy is a package that performs some basic NLP tasks for us, based on training on a large corpus. We can import spaCy libraries for different languages and of different complexities (often in small, medium, and large sizes, where larger means slower to use but with slightly more precise behaviour). For our use case, we will \"fr_core_news_sm\", which is the small French language  model.\n",
    "\n",
    "For basic questions about spaCy, refer to the documentation [here](https://spacy.io/usage/spacy-101)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences \n",
    "# nlp = spacy.load(\"en_core_web_sm\") \n",
    "nlp = spacy.load(\"fr_core_news_sm\")  #this could take a minute to download the first time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ````nlp```` object can be used to analyze documents. For example:\n",
    "\n",
    "````doc_analyzed = nlp(\"Hello world\")\n",
    "doc_analyzed\n",
    ">>> Hello world\n",
    "type(doc_analyzed)\n",
    ">>> spacy.tokens.doc.Doc\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(one_text)\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Question**: Use spaCy to list each token in the ````text```` of the document with id 1500.\n",
    "\n",
    "*Hint: spacy.tokens.doc.Doc is an iterable*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: List the POS of every token from the previous exercise. Do you agree with spaCy?\n",
    "\n",
    "*Hint: use token.pos_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lemma of a term is the simplest and most distilled version of the term, without prefixes, suffixes, etc. Converting terms to lemmas can be useful in order to reduce the vocabulary size of a corpus and group terms into equivalence classes. For example, \"run\", \"runs\", \"running\", and \"ran\" all share roughly the same semantic meaning, and in some contexts (though not all!) should be treated the same way.\n",
    "\n",
    "**Question**: extract the lemmas from the previous exercise. Which terms stay the same, and which differ? In what ways do they differ?\n",
    "\n",
    "*Hint: use token.lemma_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How many unique terms are there among all the texts? How many unique *lemmas* are there among all the texts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Plot the distribution of POS over the all the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Plot the distribution of the 100 most common words over the all the texts, provided that they are not space or punctuation. Use a log scale for the y axes and describe the graph: the y axis should be frequency, and the x axis should be words sorted by frequency.\n",
    "\n",
    "*Hint: see [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "\n",
    "Stop words are words that, under some frameworks, contribute marginal semantic value, but are important for syntax. For example, \"the\", \"and\", \"such\", etc.\n",
    "\n",
    "**Question**: In what kind of analysis can we strike stop words without worrying about losing information? In what kind of analysis might we want to keep them around?\n",
    "\n",
    "\n",
    "**Question**: Write a function:\n",
    "\n",
    "````def clean(sentence)````\n",
    "\n",
    "that strikes all stop words, replaces each remaining token with its lemma. Create a cleaned version of each abstract, and save this cleaned version as a new column of the dataframe called \"text_clean\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(sentence):\n",
    "    #todo\n",
    "    pass\n",
    "\n",
    "df_radios[\"text_clean\"] = df_radios.text.apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "spaCy calculates dependencies between different tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_doc = nlp(\"Hello! I hope you are well.\")\n",
    "exp_doc = nlp(one_text)\n",
    "for word in exp_doc:\n",
    "   print(word.text, word.tag_, word.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize this using dispacy, a visualization library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(exp_doc, style='dep', jupyter=True, options={'distance': 130})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Visualize a random title from the dataframe - describe the visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER\n",
    "\n",
    "spaCy can determine named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for ent in nlp(\"Joe Biden is the president of the USA\").ents:\n",
    "    print(\"ENT: \", ent.text, ent.label_, spacy.explain(ent.label_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: calculate the ten most common named entities and the five most common labels among the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "One of the most important statistics we can calculate for a document is the TF-IDF score. This stands for \"term frequency-inverse document frequency\" score, and it is a measure of the importance of each word to a given document, in the context of the corpus of documents. For each word in each document, we calculate the following ratio:\n",
    "\n",
    "$\\frac{TF_{word; doc}}{IDF_{word; doc}}$\n",
    "\n",
    "$TF_{word; doc} = \\frac{\\text{Total counts of word in doc}}{\\text{Number of terms in doc}}$\n",
    "\n",
    "$IDF_{word; doc} = \\log{\\left(\\frac{|\\text{corpus}|}{\\text{Number of docs containing word}}\\right)}$\n",
    "\n",
    "To have a high TF-IDF score in a document, a word must occur frequently in a document, while appearing rarely in others.\n",
    "\n",
    "**Question**: Give the limits of TF and IDF for a word/document pair, and give an example of a word-document pair where this might occur (for example, word=\"Hello\", doc=Wikipedia page for greetings, corpus=all of Wikipedia).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Code the functions TF and IDF:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(word, document):\n",
    "    #todo\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def idf(word, all_documents):\n",
    "    #todo\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def tf_idf(word, document, all_documents):\n",
    "    return tf(word, document)/idf(word, all_documents)\n",
    "\n",
    "#test your code:\n",
    "my_text = df_radios.iloc[0].text_clean\n",
    "tf_idf(\"radio\", my_text, df_radios.text_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What is the word with the highest and lowest TF-IDF score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: If we took each document and duplicated it in place, i.e.:\n",
    "\n",
    "a document \"hello world\" would become \"hello world hello world\", how would the tf-idf score of any document-word pair change?\n",
    "\n",
    "What about if instead of duplicating in place, we doubled the size of the corpus, i.e.\n",
    "\n",
    "if document \"hello world\" is in the corpus once, we add a second copy so now it is in the corpus twice. How would the tf-idf score of any document-word pair change now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What could be a potential use-case for the TF-IDF score in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Comments \n",
    "\n",
    "Add your conclusions about this lab. You can include your level of satisfation with your work and your opinion on the length and difficulty of the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
